``` cpp
#include <Wire.h>
#include <MPU6050.h>

MPU6050 imu;

// 모터 1 (X축)
#define M1_EN 6
#define M1_IN1 7
#define M1_IN2 8

// 모터 2 (Y축)
#define M2_EN 9
#define M2_IN1 10
#define M2_IN2 11

// 센서값
int16_t accX, accY, accZ;
float roll, pitch;

float roll_offset = 0.0;
float pitch_offset = 0.0;

// 시리얼 수신 버퍼
String inputString = "";
bool newData = false;

void setup() {
  Serial.begin(9600);       // 파이썬 측과 동일하게 설정
  Wire.begin();
  imu.initialize();

  pinMode(M1_EN, OUTPUT); pinMode(M1_IN1, OUTPUT); pinMode(M1_IN2, OUTPUT);
  pinMode(M2_EN, OUTPUT); pinMode(M2_IN1, OUTPUT); pinMode(M2_IN2, OUTPUT);

  if (!imu.testConnection()) {
    Serial.println("IMU 연결 실패");
    while (1);
  }

  delay(1000); // 센서 안정화

  // 초기 오프셋
  imu.getAcceleration(&accX, &accY, &accZ);
  roll_offset = atan2((float)accY, (float)accZ) * 180.0 / PI;
  pitch_offset = atan2(-(float)accX, sqrt((float)accY * accY + (float)accZ * accZ)) * 180.0 / PI;
}

void controlMotor(int in1, int in2, int en, int pwm) {
  pwm = constrain(pwm, -255, 255);
  if (pwm > 0) {
    digitalWrite(in1, HIGH);
    digitalWrite(in2, LOW);
  } else if (pwm < 0) {
    digitalWrite(in1, LOW);
    digitalWrite(in2, HIGH);
  } else {
    digitalWrite(in1, LOW);
    digitalWrite(in2, LOW);
  }
  analogWrite(en, abs(pwm));
}

float normalize_angle(float angle) {
  while (angle > 180.0) angle -= 360.0;
  while (angle < -180.0) angle += 360.0;
  return angle;
}


void loop() {
  // 시리얼 입력 처리
  while (Serial.available()) {
    char c = Serial.read();
    if (c == '\n') {
      newData = true;
      break;
    } else {
      inputString += c;
    }
  }

  // 수신된 PWM 명령 처리
  if (newData) {
    int commaIndex = inputString.indexOf(',');
    if (commaIndex > 0) {
      int pwm1 = inputString.substring(0, commaIndex).toInt();
      int pwm2 = inputString.substring(commaIndex + 1).toInt();
      controlMotor(M1_IN1, M1_IN2, M1_EN, pwm1);
      controlMotor(M2_IN1, M2_IN2, M2_EN, pwm2);
    }
    inputString = "";
    newData = false;
  }

  // IMU 측정
  imu.getAcceleration(&accX, &accY, &accZ);
  roll = normalize_angle(atan2((float)accY, (float)accZ) * 180.0 / PI - roll_offset);
  pitch = normalize_angle(atan2((float)(-accX), sqrt((float)accY * accY + (float)accZ * accZ)) * 180.0 / PI - pitch_offset);
  float gyroX = imu.getRotationX() / 131.0;
  float gyroY = imu.getRotationY() / 131.0;

  // 시리얼 출력 (Python이 받아서 obs로 사용)
  Serial.print(roll, 2); Serial.print(",");
  Serial.print(pitch, 2); Serial.print(",");
  Serial.print(gyroX, 2); Serial.print(",");
  Serial.println(gyroY, 2);

  delay(10);
}


```

``` python
import gymnasium as gym
import numpy as np
import serial
import time
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor

class BallPlateEnv(gym.Env):
    def __init__(self):
        super(BallPlateEnv, self).__init__()
        # 액션 공간: 두 모터의 PWM 값 (-100 ~ 100)
        self.action_space = gym.spaces.Box(
            low=np.array([-100, -100]),
            high=np.array([100, 100]),
            dtype=np.float32
        )
        # 관측 공간: roll, pitch, roll_gyro, pitch_gyro
        # 실제 사용되는 범위로 제한
        self.observation_space = gym.spaces.Box(
            low=np.array([-45, -45, -90, -90]),  # 각도 ±45도, 각속도 ±90도/초
            high=np.array([45, 45, 90, 90]),
            dtype=np.float32
        )
        
        # 아두이노 시리얼 통신 설정
        self.arduino = serial.Serial('COM3', 9600, timeout=1)
        time.sleep(2)  # 아두이노 초기화 대기
        self.last_action = np.zeros(2)
        
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # 아두이노 버퍼 초기화
        self.arduino.reset_input_buffer()
        # 초기 상태 읽기
        state = self._read_state()
        info = {}
        return state, info
    
    def step(self, action):
        # 마지막 액션 저장
        self.last_action = action
        
        # 액션을 아두이노로 전송 (PWM 값)
        pwm1, pwm2 = action.astype(int)
        command = f"{pwm1},{pwm2}\n"
        self.arduino.write(command.encode())
        
        # 새로운 상태 읽기
        state = self._read_state()
        
        # 보상 계산
        reward = self._calculate_reward(state)
        
        # 종료 조건
        terminated = False
        truncated = False
        
        info = {}
        
        return state, reward, terminated, truncated, info
    
    def _read_state(self):
        # 아두이노로부터 상태 읽기
        if self.arduino.in_waiting:
            try:
                data = self.arduino.readline().decode().strip()
                roll, pitch, gyro_x, gyro_y = map(float, data.split(','))
                
                # 상태값을 관측 공간 범위로 클리핑
                roll = np.clip(roll, -45, 45)
                pitch = np.clip(pitch, -45, 45)
                gyro_x = np.clip(gyro_x, -90, 90)
                gyro_y = np.clip(gyro_y, -90, 90)
                
                return np.array([roll, pitch, gyro_x, gyro_y], dtype=np.float32)
            except:
                return np.zeros(4, dtype=np.float32)
        return np.zeros(4, dtype=np.float32)
    
    def _calculate_reward(self, state):
        roll, pitch, gyro_x, gyro_y = state
        
        # 각도가 0에 가까울수록 높은 보상
        angle_reward = -(roll**2 + pitch**2) / 100.0
        
        # 각속도가 낮을수록 높은 보상
        gyro_penalty = -(gyro_x**2 + gyro_y**2) / 1000.0
        
        # 모터 제어에 대한 페널티 (에너지 효율성)
        control_penalty = -0.01 * (abs(self.last_action[0]) + abs(self.last_action[1])) / 100.0  # 100으로 나누어 정규화
        
        return angle_reward + gyro_penalty + control_penalty
    
    def close(self):
        self.arduino.close()

def make_env():
    env = BallPlateEnv()
    env = Monitor(env)
    return env

def train():
    # 환경 생성
    env = DummyVecEnv([make_env])
    
    # SAC 모델 생성
    model = SAC(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        buffer_size=100000,
        learning_starts=1000,
        batch_size=256,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        action_noise=None,
        optimize_memory_usage=False,
        ent_coef='auto',
        target_update_interval=1,
        target_entropy='auto',
        use_sde=False,
        sde_sample_freq=-1,
        use_sde_at_warmup=False,
        verbose=1
    )
    
    # 평가 콜백 설정
    eval_env = DummyVecEnv([make_env])
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./best_model",
        log_path="./logs/",
        eval_freq=1000,
        deterministic=True,
        render=False
    )
    
    # 학습 시작
    model.learn(
        total_timesteps=1000000,
        callback=eval_callback,
        log_interval=100
    )
    
    # 모델 저장
    model.save("ball_plate_sac")
    
    # 환경 종료
    env.close()
    eval_env.close()

if __name__ == "__main__":
    train()

```